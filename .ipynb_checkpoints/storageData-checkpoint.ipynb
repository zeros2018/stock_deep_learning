{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe0e084-0eeb-4108-942e-054b69fa4eaf",
   "metadata": {},
   "source": [
    "# Storage Data from Hadoop to Hive Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f75c3-5e19-4ca6-ad32-667d973b4419",
   "metadata": {},
   "source": [
    "The main idea of this Jupyter is to connect to hadoop and read the data using Spark in order to store it in a proper database. In this case I will use Apache Hive Database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d4418-50a9-4048-b457-b24c398f6055",
   "metadata": {},
   "source": [
    "First we import the libraries that we will use throughout the jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfaa3c25-67c3-4a7a-a0f1-ebf862e054f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries from Spark\n",
    "from pyspark.sql import SparkSession # Spark Session for working with hadoop files\n",
    "from pyspark.sql.functions import lit  # functions for working with text\n",
    "from pyspark.sql.functions import col, sum # functions for columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fac7a4-6ffd-439a-ae86-82dda8802de9",
   "metadata": {},
   "source": [
    "## Configurate Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0d48f-8d42-4ec1-bf45-964bc5f58aff",
   "metadata": {},
   "source": [
    "Configure use spark session to connect to the hive database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9acb77a0-f2f5-4992-a7fa-59db1bb7b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration before creating the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockTweetsToHive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2413513-16d4-4d8e-be8b-65563efe6d3c",
   "metadata": {},
   "source": [
    "## Directories in Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3f609-fbe3-4051-a4f5-42deff67b7a2",
   "metadata": {},
   "source": [
    "We create the directories where our data are located, in this case in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c10c3ac7-e8de-4b90-bb4c-7a746a6db042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "# tweets\n",
    "dir_tweets = '/data/tweets/stocktweet.csv'\n",
    "\n",
    "# stocks\n",
    "set_cvs = [\n",
    "    (\"/data/companies/BAC.csv\", \"BAC\"),\n",
    "    (\"/data/companies/DIS.csv\", \"DIS\"),\n",
    "    (\"/data/companies/PG.csv\", \"PG\"),\n",
    "    (\"/data/companies/TSLA.csv\", \"TSLA\"),\n",
    "    (\"/data/companies/WMT.csv\", \"WMT\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62943f4d-c29d-47db-b6a1-35bca4388108",
   "metadata": {},
   "source": [
    "## Reading tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0b71b-39c1-43c5-af66-aa7bccc4d6a7",
   "metadata": {},
   "source": [
    "In order to be able to read tweets that sometimes contain data that could obstruct some internal spark functions, we must configure the function to read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3437f04c-3dce-413e-9838-619556308c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+--------------------+\n",
      "|    id|      date|ticker|               tweet|\n",
      "+------+----------+------+--------------------+\n",
      "|100001|01/01/2020|  AMZN|$AMZN Dow futures...|\n",
      "|100002|01/01/2020|  TSLA|$TSLA Daddy's dri...|\n",
      "|100003|01/01/2020|  AAPL|$AAPL Weâ€™ll been ...|\n",
      "|100004|01/01/2020|  TSLA|$TSLA happy new y...|\n",
      "|100005|01/01/2020|  TSLA|\"$TSLA haha just ...|\n",
      "|100006|01/01/2020|  TSLA|$TSLA NOBODY: Gas...|\n",
      "|100007|02/01/2020|  AAPL|$AAPL $300 calls ...|\n",
      "|100008|02/01/2020|  AAPL|$AAPL Remember, i...|\n",
      "|100009|02/01/2020|  AAPL|$AAPL called it, ...|\n",
      "|100010|02/01/2020|    HD|$HD Bought more a...|\n",
      "|100011|02/01/2020|  AAPL|Apple is taking t...|\n",
      "|100012|02/01/2020|  AAPL|$AAPL not a bad d...|\n",
      "|100013|02/01/2020|  AAPL|$AAPL where are a...|\n",
      "|100014|03/01/2020|  NVDA|$NVDA This should...|\n",
      "|100015|03/01/2020|  AAPL|$AAPL tomorrow bu...|\n",
      "|100016|03/01/2020|  AAPL|$AAPL Thanks for ...|\n",
      "|100017|03/01/2020|  AAPL|$AAPL leave enoug...|\n",
      "|100018|03/01/2020|  AAPL|$AAPL short this ...|\n",
      "|100019|03/01/2020|  TSLA|$TSLA  I must ris...|\n",
      "|100020|03/01/2020|  TSLA|$TSLA shorts are ...|\n",
      "+------+----------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "10003\n"
     ]
    }
   ],
   "source": [
    "df_tweets = spark.read.option(\"header\", \"true\") \\\n",
    "                     .option(\"inferSchema\", \"true\") \\\n",
    "                     .option(\"multiLine\", \"true\") \\\n",
    "                     .option(\"encoding\", \"UTF-8\") \\\n",
    "                     .option(\"delimiter\", \",\") \\\n",
    "                    .csv(dir_tweets, header=True, inferSchema=True)\n",
    "\n",
    "df_tweets.show()\n",
    "print(df_tweets.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a5815-2910-4d96-850d-962b2f88aacd",
   "metadata": {},
   "source": [
    "## Counting Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cc713-ca69-40ec-94db-d3844dfc0c44",
   "metadata": {},
   "source": [
    "Because the function to read the data is not efficient we need to control the nulls that were registered due to formatting problems. The following is a list of the nulls by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b41cc5de-acfa-4e6a-b10b-be19f92f01fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-----+\n",
      "| id|date|ticker|tweet|\n",
      "+---+----+------+-----+\n",
      "|  0|   2|     2|    3|\n",
      "+---+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame named df\n",
    "# We count the null values for each column\n",
    "null_counts = df_tweets.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_tweets.columns])\n",
    "\n",
    "# Show the count of null values for each column\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5255b7-c8fb-4efb-93c7-a653658d37e1",
   "metadata": {},
   "source": [
    "## Showing Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d5c1d-abff-4bef-a006-c059c576ea3c",
   "metadata": {},
   "source": [
    "Here I am checking where the nulls are by columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c5b674-300b-4444-8af1-a2a2ccf1385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_nulls_date = df_tweets.filter(df_tweets['date'].isNull())\n",
    "df_tweet_nulls_ticker = df_tweets.filter(df_tweets['ticker'].isNull())\n",
    "df_tweet_nulls_comment = df_tweets.filter(df_tweets['tweet'].isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4caea036-aec8-498e-89c0-834522dc1fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------+-----+\n",
      "|                  id|date|ticker|tweet|\n",
      "+--------------------+----+------+-----+\n",
      "|And what did the ...|null|  null| null|\n",
      "|             ðŸ˜·ðŸ˜·ðŸ˜·\"|null|  null| null|\n",
      "+--------------------+----+------+-----+\n",
      "\n",
      "2\n",
      "+--------------------+----+------+-----+\n",
      "|                  id|date|ticker|tweet|\n",
      "+--------------------+----+------+-----+\n",
      "|And what did the ...|null|  null| null|\n",
      "|             ðŸ˜·ðŸ˜·ðŸ˜·\"|null|  null| null|\n",
      "+--------------------+----+------+-----+\n",
      "\n",
      "2\n",
      "+--------------------+-------------+--------------------+-----+\n",
      "|                  id|         date|              ticker|tweet|\n",
      "+--------------------+-------------+--------------------+-----+\n",
      "|And what did the ...|         null|                null| null|\n",
      "|             ðŸ˜·ðŸ˜·ðŸ˜·\"|         null|                null| null|\n",
      "|                Well| there you go| crash accordingl...| null|\n",
      "+--------------------+-------------+--------------------+-----+\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "df_tweet_nulls_date.show()\n",
    "print(df_tweet_nulls_date.count())\n",
    "df_tweet_nulls_ticker.show()\n",
    "print(df_tweet_nulls_ticker.count())\n",
    "df_tweet_nulls_comment.show()\n",
    "print(df_tweet_nulls_comment.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c465f96-88de-433a-a4be-ab7394c5c6ea",
   "metadata": {},
   "source": [
    "## Dropping nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8a11d-cbec-47fb-a163-35196499a7cf",
   "metadata": {},
   "source": [
    "I need to eliminate the nulls that do not correspond to the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c68bbc-8a68-43e4-b427-703eb9519966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+--------------------+\n",
      "|    id|      date|ticker|               tweet|\n",
      "+------+----------+------+--------------------+\n",
      "|100001|01/01/2020|  AMZN|$AMZN Dow futures...|\n",
      "|100002|01/01/2020|  TSLA|$TSLA Daddy's dri...|\n",
      "|100003|01/01/2020|  AAPL|$AAPL Weâ€™ll been ...|\n",
      "|100004|01/01/2020|  TSLA|$TSLA happy new y...|\n",
      "|100005|01/01/2020|  TSLA|\"$TSLA haha just ...|\n",
      "|100006|01/01/2020|  TSLA|$TSLA NOBODY: Gas...|\n",
      "|100007|02/01/2020|  AAPL|$AAPL $300 calls ...|\n",
      "|100008|02/01/2020|  AAPL|$AAPL Remember, i...|\n",
      "|100009|02/01/2020|  AAPL|$AAPL called it, ...|\n",
      "|100010|02/01/2020|    HD|$HD Bought more a...|\n",
      "|100011|02/01/2020|  AAPL|Apple is taking t...|\n",
      "|100012|02/01/2020|  AAPL|$AAPL not a bad d...|\n",
      "|100013|02/01/2020|  AAPL|$AAPL where are a...|\n",
      "|100014|03/01/2020|  NVDA|$NVDA This should...|\n",
      "|100015|03/01/2020|  AAPL|$AAPL tomorrow bu...|\n",
      "|100016|03/01/2020|  AAPL|$AAPL Thanks for ...|\n",
      "|100017|03/01/2020|  AAPL|$AAPL leave enoug...|\n",
      "|100018|03/01/2020|  AAPL|$AAPL short this ...|\n",
      "|100019|03/01/2020|  TSLA|$TSLA  I must ris...|\n",
      "|100020|03/01/2020|  TSLA|$TSLA shorts are ...|\n",
      "+------+----------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "df_tweets = df_tweets.dropna()\n",
    "df_tweets.show()\n",
    "print(df_tweets.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851dee4-2f63-435e-93ea-f51921a2527d",
   "metadata": {},
   "source": [
    "## Reading Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d317f-9452-4b05-9f0d-2d46b75d81f7",
   "metadata": {},
   "source": [
    "\n",
    "To read the share prices I have to create a new column in this case company in order to identify the records of each cvs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "560da232-02c8-462e-846e-acdbb67b23ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+------------------+--------+-------+\n",
      "|      Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|company|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+--------+-------+\n",
      "|2019-12-31|35.029998779296875|  35.2599983215332|34.970001220703125|35.220001220703125|31.893484115600586|29630100|    BAC|\n",
      "|2020-01-02|35.349998474121094| 35.65999984741211|35.290000915527344| 35.63999938964844|32.273807525634766|37614200|    BAC|\n",
      "|2020-01-03| 34.97999954223633|35.150001525878906|  34.7599983215332|34.900001525878906|31.603710174560547|50357900|    BAC|\n",
      "|2020-01-06| 34.40999984741211|34.900001525878906|34.369998931884766|34.849998474121094|31.558433532714844|42185000|    BAC|\n",
      "|2020-01-07| 34.70000076293945| 34.91999816894531|34.529998779296875|34.619998931884766| 31.35015296936035|34149000|    BAC|\n",
      "|2020-01-08| 34.56999969482422|35.189998626708984| 34.54999923706055|34.970001220703125|31.667097091674805|45311600|    BAC|\n",
      "|2020-01-09| 35.29999923706055| 35.33000183105469|34.939998626708984|35.029998779296875|31.721426010131836|39861600|    BAC|\n",
      "|2020-01-10|              35.0| 35.06999969482422| 34.65999984741211|  34.7400016784668| 31.45882225036621|39730400|    BAC|\n",
      "|2020-01-13| 34.84000015258789| 35.06999969482422| 34.65999984741211|35.060001373291016| 31.74859046936035|37956100|    BAC|\n",
      "|2020-01-14| 35.29999923706055| 35.66999816894531| 35.11000061035156| 35.31999969482422| 31.98404312133789|66719300|    BAC|\n",
      "|2020-01-15| 34.77000045776367|34.790000915527344| 34.34000015258789| 34.66999816894531| 31.39542579650879|74745500|    BAC|\n",
      "|2020-01-16|34.900001525878906|  34.9900016784668|34.599998474121094|34.720001220703125|31.440710067749023|50368500|    BAC|\n",
      "|2020-01-17| 34.91999816894531| 34.95000076293945|34.599998474121094|34.709999084472656|31.431655883789062|54156100|    BAC|\n",
      "|2020-01-21| 34.41999816894531| 34.52000045776367|34.220001220703125|  34.2599983215332|31.024145126342773|50811000|    BAC|\n",
      "|2020-01-22|34.369998931884766| 34.45000076293945| 34.22999954223633| 34.36000061035156|31.114707946777344|38879700|    BAC|\n",
      "|2020-01-23| 34.09000015258789|  34.2599983215332| 33.72999954223633|34.119998931884766|30.897377014160156|45717100|    BAC|\n",
      "|2020-01-24|34.119998931884766| 34.13999938964844| 33.27000045776367|33.540000915527344|30.372159957885742|47797200|    BAC|\n",
      "|2020-01-27| 32.56999969482422|33.130001068115234|32.470001220703125|32.849998474121094| 29.74732780456543|57096200|    BAC|\n",
      "|2020-01-28| 33.02000045776367|  33.4900016784668| 32.95000076293945|  33.2400016784668|30.100494384765625|38834600|    BAC|\n",
      "|2020-01-29| 33.33000183105469|33.439998626708984| 32.97999954223633|  33.0099983215332|  29.8922176361084|37414700|    BAC|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1270\n"
     ]
    }
   ],
   "source": [
    "# Create a list of DataFrames with the column 'company'.\n",
    "dataframes = []\n",
    "\n",
    "for cvs, company in set_cvs:\n",
    "    # Load the DataFrame from the CSV file\n",
    "    df = spark.read.csv(cvs, header=True, inferSchema=True)\n",
    "    \n",
    "    # Add a 'company' column with the company name\n",
    "    df_with_company = df.withColumn(\"company\", lit(company))\n",
    "    \n",
    "    # Add the DataFrame with the new column to the list\n",
    "    dataframes.append(df_with_company)\n",
    "\n",
    "# Union all DataFrames into one\n",
    "df_stock = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    df_stock = df_stock.union(df)\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_stock.show()\n",
    "print(df_stock.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3742a2c-eaa6-4d14-a0e5-0ca3be0f98ab",
   "metadata": {},
   "source": [
    "## Saving registers in Hive DataBase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7396b92-6b60-48f3-adb2-13dedc3a7601",
   "metadata": {},
   "source": [
    "Before we can save data to the database, we must make sure that the database is turned on and has been correctly configured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb8c7b-c837-4443-a8d0-dfff0275a683",
   "metadata": {},
   "source": [
    "### Showing databases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39bef87e-8b4e-45fc-8c6f-62d280848432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|benchmark|\n",
      "|      ca2|\n",
      "|  default|\n",
      "| testhive|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Show databases;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe424c-739e-4ba5-ba55-bb946092b872",
   "metadata": {},
   "source": [
    "### Creating a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7e16b00-ef56-4adc-9c8e-7449b7782c59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ca2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fad9b54-40ae-4d8a-a2a9-0a3e0564bc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|benchmark|\n",
      "|      ca2|\n",
      "|  default|\n",
      "| testhive|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Show databases;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86655f92-4bc9-4acf-91cd-d06adf9fb8f0",
   "metadata": {},
   "source": [
    "#### Using the Database created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61975782-6397-4f8f-95c2-a8ba9432d16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE ca2;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a00468-aee1-4377-8937-1841b3c4b01a",
   "metadata": {},
   "source": [
    "#### Writing the data to database hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b2cc673-5831-49cd-bbb3-da895f1ccf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_tweets.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"ca2.tweets\")  # Name of the database and table in Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "121218a4-e51a-4135-b729-e3b64f0b472c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+--------------------+\n",
      "|    id|      date|ticker|               tweet|\n",
      "+------+----------+------+--------------------+\n",
      "|100001|01/01/2020|  AMZN|$AMZN Dow futures...|\n",
      "|100002|01/01/2020|  TSLA|$TSLA Daddy's dri...|\n",
      "|100003|01/01/2020|  AAPL|$AAPL Weâ€™ll been ...|\n",
      "|100004|01/01/2020|  TSLA|$TSLA happy new y...|\n",
      "|100005|01/01/2020|  TSLA|\"$TSLA haha just ...|\n",
      "|100006|01/01/2020|  TSLA|$TSLA NOBODY: Gas...|\n",
      "|100007|02/01/2020|  AAPL|$AAPL $300 calls ...|\n",
      "|100008|02/01/2020|  AAPL|$AAPL Remember, i...|\n",
      "|100009|02/01/2020|  AAPL|$AAPL called it, ...|\n",
      "|100010|02/01/2020|    HD|$HD Bought more a...|\n",
      "|100011|02/01/2020|  AAPL|Apple is taking t...|\n",
      "|100012|02/01/2020|  AAPL|$AAPL not a bad d...|\n",
      "|100013|02/01/2020|  AAPL|$AAPL where are a...|\n",
      "|100014|03/01/2020|  NVDA|$NVDA This should...|\n",
      "|100015|03/01/2020|  AAPL|$AAPL tomorrow bu...|\n",
      "|100016|03/01/2020|  AAPL|$AAPL Thanks for ...|\n",
      "|100017|03/01/2020|  AAPL|$AAPL leave enoug...|\n",
      "|100018|03/01/2020|  AAPL|$AAPL short this ...|\n",
      "|100019|03/01/2020|  TSLA|$TSLA  I must ris...|\n",
      "|100020|03/01/2020|  TSLA|$TSLA shorts are ...|\n",
      "+------+----------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from tweets;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1ec115a-448d-42fb-9ae1-782774e39e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   10000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select COUNT(*) from ca2.tweets;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d30e3c-8e8c-4f12-a978-981dcdc7d679",
   "metadata": {},
   "source": [
    "### Writing the data to database hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1be3dfbf-0f3b-4f5c-9208-fcb2a34f65ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 15:27:38 WARN MemoryManager: Total allocation exceeds 50.00% (522,977,280 bytes) of heap memory\n",
      "Scaling row group sizes to 97.41% for 4 writers\n",
      "24/12/10 15:27:38 WARN MemoryManager: Total allocation exceeds 50.00% (522,977,280 bytes) of heap memory\n",
      "Scaling row group sizes to 77.93% for 5 writers\n",
      "24/12/10 15:27:38 WARN MemoryManager: Total allocation exceeds 50.00% (522,977,280 bytes) of heap memory\n",
      "Scaling row group sizes to 97.41% for 4 writers\n"
     ]
    }
   ],
   "source": [
    "df_stock.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"ca2.prices\")  # Name of the database and table in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd2012-45ff-43b5-9369-4c28ee420a12",
   "metadata": {},
   "source": [
    "### Showing the dato from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6c40b88-fac3-4e05-ac9f-511e00d7230a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+------------------+---------+-------+\n",
      "|      Date|              Open|              High|               Low|             Close|         Adj Close|   Volume|company|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+---------+-------+\n",
      "|2019-12-31|              27.0|28.086000442504883| 26.80533218383789|  27.8886661529541|  27.8886661529541|154285500|   TSLA|\n",
      "|2020-01-02|28.299999237060547|28.713333129882812| 28.11400032043457| 28.68400001525879| 28.68400001525879|142981500|   TSLA|\n",
      "|2020-01-03|29.366666793823242|30.266666412353516|29.128000259399414|29.534000396728516|29.534000396728516|266677500|   TSLA|\n",
      "|2020-01-06|29.364667892456055|30.104000091552734|29.333332061767578|  30.1026668548584|  30.1026668548584|151995000|   TSLA|\n",
      "|2020-01-07|30.760000228881836|31.441999435424805|30.224000930786133|31.270666122436523|31.270666122436523|268231500|   TSLA|\n",
      "|2020-01-08|31.579999923706055|   33.232666015625|31.215333938598633| 32.80933380126953| 32.80933380126953|467164500|   TSLA|\n",
      "|2020-01-09| 33.13999938964844|33.253334045410156|31.524667739868164|32.089332580566406|32.089332580566406|426606000|   TSLA|\n",
      "|2020-01-10| 32.11933135986328|  32.3293342590332|31.579999923706055|31.876667022705078|31.876667022705078|194392500|   TSLA|\n",
      "|2020-01-13|32.900001525878906| 35.04199981689453| 32.79999923706055|34.990665435791016|34.990665435791016|397764000|   TSLA|\n",
      "|2020-01-14|36.284000396728516| 36.49399948120117| 34.99333190917969|35.861331939697266|35.861331939697266|434943000|   TSLA|\n",
      "|2020-01-15| 35.31733322143555| 35.85599899291992|34.452667236328125| 34.56666564941406| 34.56666564941406|260532000|   TSLA|\n",
      "|2020-01-16| 32.91666793823242|34.297332763671875| 32.81133270263672|   34.232666015625|   34.232666015625|326050500|   TSLA|\n",
      "|2020-01-17|33.840667724609375| 34.37799835205078| 33.54399871826172| 34.03333282470703| 34.03333282470703|204436500|   TSLA|\n",
      "|2020-01-21|35.349998474121094|36.571998596191406|35.227333068847656| 36.47999954223633| 36.47999954223633|267052500|   TSLA|\n",
      "|2020-01-22|38.125999450683594|39.633331298828125| 37.27333450317383| 37.97066879272461| 37.97066879272461|470535000|   TSLA|\n",
      "|2020-01-23|37.616668701171875| 38.79999923706055|37.040000915527344| 38.14666748046875| 38.14666748046875|294765000|   TSLA|\n",
      "|2020-01-24| 38.04199981689453| 38.25733184814453| 36.95066833496094|37.654666900634766|37.654666900634766|215304000|   TSLA|\n",
      "|2020-01-27|36.132667541503906| 37.62933349609375| 35.95199966430664|37.201332092285156|37.201332092285156|204121500|   TSLA|\n",
      "|2020-01-28| 37.89933395385742| 38.45399856567383|  37.2053337097168|37.793331146240234|37.793331146240234|176827500|   TSLA|\n",
      "|2020-01-29| 38.37933349609375| 39.31999969482422| 37.82866668701172|   38.732666015625|   38.732666015625|267022500|   TSLA|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from ca2.prices;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87d4c458-dc97-43c7-98ba-de62885c6e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1270|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select COUNT(*) from ca2.prices;\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
